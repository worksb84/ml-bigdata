{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T04:30:38.880827Z",
     "iopub.status.busy": "2024-03-15T04:30:38.880508Z",
     "iopub.status.idle": "2024-03-15T04:32:39.047245Z",
     "shell.execute_reply": "2024-03-15T04:32:39.046430Z",
     "shell.execute_reply.started": "2024-03-15T04:30:38.880801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0dfc19e5fe447b91d48f024257ff74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 50 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "24/03/15 04:30:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/15 04:30:41 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-10-0-15-66.ap-northeast-2.compute.internal/10.0.15.66:8032\n",
      "24/03/15 04:30:42 INFO Configuration: resource-types.xml not found\n",
      "24/03/15 04:30:42 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "24/03/15 04:30:42 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n",
      "24/03/15 04:30:42 INFO Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "24/03/15 04:30:42 INFO Client: Setting up container launch context for our AM\n",
      "24/03/15 04:30:42 INFO Client: Setting up the launch environment for our AM container\n",
      "24/03/15 04:30:42 INFO Client: Preparing resources for our AM container\n",
      "24/03/15 04:30:42 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/03/15 04:30:44 INFO Client: Uploading resource file:/mnt/tmp/spark-18893b6d-3736-42ba-926c-014860e9d4df/__spark_libs__2335498928672892509.zip -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/__spark_libs__2335498928672892509.zip\n",
      "24/03/15 04:30:45 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/arpack_combined_all-0.1.jar -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/arpack_combined_all-0.1.jar\n",
      "24/03/15 04:30:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/core-1.1.2.jar -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/core-1.1.2.jar\n",
      "24/03/15 04:30:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/jniloader-1.1.jar -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/jniloader-1.1.jar\n",
      "24/03/15 04:30:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.7.1-incubating.jar -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/livy-api-0.7.1-incubating.jar\n",
      "24/03/15 04:30:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.7.1-incubating.jar -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/livy-rsc-0.7.1-incubating.jar\n",
      "24/03/15 04:30:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.1-incubating.jar -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/livy-thriftserver-session-0.7.1-incubating.jar\n",
      "24/03/15 04:30:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/native_ref-java-1.1.jar -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/native_ref-java-1.1.jar\n",
      "24/03/15 04:30:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/native_system-java-1.1.jar -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/native_system-java-1.1.jar\n",
      "24/03/15 04:30:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netlib-native_ref-linux-armhf-1.1-natives.jar -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/netlib-native_ref-linux-armhf-1.1-natives.jar\n",
      "24/03/15 04:30:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netlib-native_ref-linux-i686-1.1-natives.jar -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/netlib-native_ref-linux-i686-1.1-natives.jar\n",
      "24/03/15 04:30:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netlib-native_ref-linux-x86_64-1.1-natives.jar -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/netlib-native_ref-linux-x86_64-1.1-natives.jar\n",
      "24/03/15 04:30:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netlib-native_ref-osx-x86_64-1.1-natives.jar -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/netlib-native_ref-osx-x86_64-1.1-natives.jar\n",
      "24/03/15 04:30:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netlib-native_ref-win-i686-1.1-natives.jar -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/netlib-native_ref-win-i686-1.1-natives.jar\n",
      "24/03/15 04:30:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netlib-native_ref-win-x86_64-1.1-natives.jar -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/netlib-native_ref-win-x86_64-1.1-natives.jar\n",
      "24/03/15 04:30:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netlib-native_system-linux-armhf-1.1-natives.jar -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/netlib-native_system-linux-armhf-1.1-natives.jar\n",
      "24/03/15 04:30:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netlib-native_system-linux-i686-1.1-natives.jar -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/netlib-native_system-linux-i686-1.1-natives.jar\n",
      "24/03/15 04:30:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netlib-native_system-linux-x86_64-1.1-natives.jar -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/netlib-native_system-linux-x86_64-1.1-natives.jar\n",
      "24/03/15 04:30:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netlib-native_system-osx-x86_64-1.1-natives.jar -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/netlib-native_system-osx-x86_64-1.1-natives.jar\n",
      "24/03/15 04:30:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netlib-native_system-win-i686-1.1-natives.jar -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/netlib-native_system-win-i686-1.1-natives.jar\n",
      "24/03/15 04:30:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netlib-native_system-win-x86_64-1.1-natives.jar -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/netlib-native_system-win-x86_64-1.1-natives.jar\n",
      "24/03/15 04:30:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/netty-all-4.1.17.Final.jar\n",
      "24/03/15 04:30:46 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/commons-codec-1.9.jar\n",
      "24/03/15 04:30:46 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.1-incubating.jar -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/livy-core_2.12-0.7.1-incubating.jar\n",
      "24/03/15 04:30:46 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.1-incubating.jar -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/livy-repl_2.12-0.7.1-incubating.jar\n",
      "24/03/15 04:30:46 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/hudi-defaults.conf\n",
      "24/03/15 04:30:47 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/sparkr.zip\n",
      "24/03/15 04:30:47 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/pyspark.zip\n",
      "24/03/15 04:30:47 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/py4j-0.10.9.5-src.zip\n",
      "24/03/15 04:30:47 INFO Client: Uploading resource file:/mnt/tmp/spark-18893b6d-3736-42ba-926c-014860e9d4df/__spark_conf__7295906378714313389.zip -> hdfs://ip-10-0-15-66.ap-northeast-2.compute.internal:8020/user/livy/.sparkStaging/application_1707107681556_0051/__spark_conf__.zip\n",
      "24/03/15 04:30:47 INFO SecurityManager: Changing view acls to: livy\n",
      "24/03/15 04:30:47 INFO SecurityManager: Changing modify acls to: livy\n",
      "24/03/15 04:30:47 INFO SecurityManager: Changing view acls groups to: \n",
      "24/03/15 04:30:47 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/03/15 04:30:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "24/03/15 04:30:47 INFO Client: Submitting application application_1707107681556_0051 to ResourceManager\n",
      "24/03/15 04:30:47 INFO YarnClientImpl: Submitted application application_1707107681556_0051\n",
      "24/03/15 04:30:47 INFO Client: Application report for application_1707107681556_0051 (state: ACCEPTED)\n",
      "24/03/15 04:30:47 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [Fri Mar 15 04:30:47 +0000 2024] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:24576, vCores:8> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 85.41667 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:24576, vCores:8> ; Queue's used capacity (absolute resource) = <memory:20992, vCores:5> ; Queue's max capacity (absolute resource) = <memory:24576, vCores:8> ; \n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1710477047485\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-10-0-15-66.ap-northeast-2.compute.internal:20888/proxy/application_1707107681556_0051/\n",
      "\t user: livy\n",
      "24/03/15 04:30:47 INFO ShutdownHookManager: Shutdown hook called\n",
      "24/03/15 04:30:47 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-18893b6d-3736-42ba-926c-014860e9d4df\n",
      "24/03/15 04:30:47 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-c3c7d466-65f0-42be-985c-8d20ede1f7b7\n",
      "\n",
      "YARN Diagnostics: \n",
      "[Fri Mar 15 04:30:47 +0000 2024] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:24576, vCores:8> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 85.41667 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:24576, vCores:8> ; Queue's used capacity (absolute resource) = <memory:20992, vCores:5> ; Queue's max capacity (absolute resource) = <memory:24576, vCores:8> ; .\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions._\n",
    "import scala.util.{Failure, Success, Try}\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "import java.time.LocalDateTime\n",
    "import java.time.LocalDate\n",
    "import java.time.format.DateTimeFormatter\n",
    "import java.time.temporal.ChronoUnit.DAYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateTimeFormatter = DateTimeFormatter.ofPattern(\"yyyyMMddHHmmss\")\n",
    "def appName: String = LocalDateTime.now().format(dateTimeFormatter)\n",
    "def previousDay(minusDays: Long) = {\n",
    "    LocalDateTime.now().minusDays(minusDays).format(dateTimeFormatter).substring(0, 8)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val appName = LocalDateTime.now().format(DateTimeFormatter.ofPattern(\"yyyyMMddHHmmss\"))\n",
    "\n",
    "val spark = (\n",
    "    SparkSession\n",
    "    .builder()\n",
    "    .appName(appName)\n",
    "    .config(\"spark.cores.max\", 8)\n",
    "    .config(\"spark.executor.cores\", 8)\n",
    "    .config(\"spark.executor.memory\", \"36g\")\n",
    "    .getOrCreate()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val mongoUrl = \"mongodb+srv://xxxxxxxxxxxxxxxxxxxxxx/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val fsDf = (\n",
    "    spark.read.format(\"mongodb\")\n",
    "    .option(\"spark.mongodb.read.connection.uri\", mongoUrl)\n",
    "    .option(\"spark.mongodb.write.connection.uri\", mongoUrl)\n",
    "    .option(\"database\", \"coreEngine\")\n",
    "    .option(\"collection\", \"ReportRiskPremiumCalcurate\")\n",
    "    .option(\"aggregation.pipeline\", \"\"\"\n",
    "[\n",
    "  {\n",
    "    $group: {\n",
    "      _id: \"$stockCode\",\n",
    "      stock: {\n",
    "        $top: {\n",
    "          output: \"$$ROOT\",\n",
    "          sortBy: { updateDate: -1 },\n",
    "        },\n",
    "      },\n",
    "    },\n",
    "  },\n",
    "  { $replaceRoot: { newRoot: \"$stock\" } },\n",
    "]\n",
    "\"\"\")\n",
    "    .load()\n",
    "    .drop(\"_id\", \"updateDate\")\n",
    "    .withColumnRenamed(\"riskPremium\", \"FSRiskPremium\")\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val nsDf = (\n",
    "    spark.read.format(\"mongodb\")\n",
    "    .option(\"spark.mongodb.read.connection.uri\", mongoUrl)\n",
    "    .option(\"spark.mongodb.write.connection.uri\", mongoUrl)\n",
    "    .option(\"database\", \"coreEngine\")\n",
    "    .option(\"collection\", \"NewsRiskPremium\")\n",
    "    .load()\n",
    "    .drop(\"_id\", \"riskParagraph\", \"wholeParagraph\")\n",
    "    .withColumnRenamed(\"riskPremium\", \"NSRiskPremium\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val hfDf = (\n",
    "    spark.read.format(\"mongodb\")\n",
    "    .option(\"spark.mongodb.read.connection.uri\", mongoUrl)\n",
    "    .option(\"spark.mongodb.write.connection.uri\", mongoUrl)\n",
    "    .option(\"database\", \"coreEngine\")\n",
    "    .option(\"collection\", \"PriceRiskPremium\")\n",
    "    .load()\n",
    "    .drop(\"_id\", \"stockFullName\")\n",
    "    .withColumnRenamed(\"riskPremium\", \"HFRiskPremium\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val matchPipe = String.format(\"{ $match: { updateDate: { $gte: '%s' } } }\", previousDay(1555))\n",
    "val indexDf = (\n",
    "    spark.read.format(\"mongodb\")\n",
    "    .option(\"spark.mongodb.read.connection.uri\", mongoUrl)\n",
    "    .option(\"spark.mongodb.write.connection.uri\", mongoUrl)\n",
    "    .option(\"database\", \"coreEngine\")\n",
    "    .option(\"collection\", \"IndexComposition\")\n",
    "    .option(\"aggregation.pipeline\", matchPipe)\n",
    "    .load()\n",
    "    .select(\"updateDate\", \"rank\", \"index\", \"isuSrtCd\")\n",
    "    .withColumnRenamed(\"isuSrtCd\", \"stockCode\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val kosdaq150Df = indexDf.where(col(\"index\") === \"kosdaq_150\").withColumnRenamed(\"rank\", \"kosdaq150\").drop(\"rank\").drop(\"index\")\n",
    "val kospi200Df = indexDf.where(col(\"index\") === \"kospi_200\").withColumnRenamed(\"rank\", \"kospi200\").drop(\"rank\").drop(\"index\")\n",
    "val krx100Df = indexDf.where(col(\"index\") === \"krx_100\").withColumnRenamed(\"rank\", \"krx100\").drop(\"rank\").drop(\"index\")\n",
    "val krx300Df = indexDf.where(col(\"index\") === \"krx_300\").withColumnRenamed(\"rank\", \"krx300\").drop(\"rank\").drop(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val riskPremiumDf = (\n",
    "    hfDf\n",
    "    .join(fsDf, Seq(\"stockCode\"), \"left\")\n",
    "    .join(nsDf, Seq(\"stockCode\", \"updateDate\"), \"left\")\n",
    "    .na.fill(0)\n",
    "    .withColumn(\"BDRiskPremium\", lit(0))\n",
    "    .withColumn(\"FNRiskPremium\", lit(0))\n",
    "    .withColumn(\"HFPctRank\", percent_rank().over(Window.partitionBy(\"updateDate\").orderBy(\"HFRiskPremium\")))\n",
    "    .withColumn(\"BDPctRank\", percent_rank().over(Window.partitionBy(\"updateDate\").orderBy(\"BDRiskPremium\")))\n",
    "    .withColumn(\"FNPctRank\", percent_rank().over(Window.partitionBy(\"updateDate\").orderBy(\"FNRiskPremium\")))\n",
    "    .withColumn(\"NSPctRank\", percent_rank().over(Window.partitionBy(\"updateDate\").orderBy(\"NSRiskPremium\")))\n",
    "    .withColumn(\"riskPremium\", col(\"FSRiskPremium\") + (col(\"HFRiskPremium\") * lit(0.1)) + (col(\"BDRiskPremium\") * lit(0.1)) + (col(\"FNRiskPremium\") * lit(0.1)) + (col(\"NSRiskPremium\") * lit(0.1)))\n",
    "    .withColumn(\"prevRiskPremium\", lag(col(\"riskPremium\"), offset=-1).over(Window.partitionBy(\"stockCode\").orderBy([\"updateDate\"])) - col(\"riskPremium\"))\n",
    "    .where(col(\"stockName\").isNotNull)\n",
    "    .withColumn(\"score\", ((lit(1) - col(\"FSPctRank\")) * 10000).cast(IntegerType))\n",
    "    .withColumn(\"FSScore\", ((lit(1) - col(\"FSPctRank\")) * 10000).cast(IntegerType))\n",
    "    .withColumn(\"HFScore\", ((lit(1) - col(\"HFPctRank\")) * 10000).cast(IntegerType))\n",
    "    .withColumn(\"BDScore\", ((lit(1) - col(\"BDPctRank\")) * 10000).cast(IntegerType))\n",
    "    .withColumn(\"FNScore\", ((lit(1) - col(\"FNPctRank\")) * 10000).cast(IntegerType))\n",
    "    .withColumn(\"NSScore\", ((lit(1) - col(\"NSPctRank\")) * 10000).cast(IntegerType))\n",
    "    .withColumn(\"ReRank\", row_number().over(Window.partitionBy(col(\"updateDate\")).orderBy(col(\"grade\"), col(\"FSPctRank\"))))\n",
    "    .join(kosdaq150Df, Seq(\"stockCode\", \"updateDate\"), \"left\")\n",
    "    .join(kospi200Df, Seq(\"stockCode\", \"updateDate\"), \"left\")\n",
    "    .join(krx100Df, Seq(\"stockCode\", \"updateDate\"), \"left\")\n",
    "    .join(krx300Df, Seq(\"stockCode\", \"updateDate\"), \"left\")\n",
    "    .withColumn(\"fs\", struct(col(\"FSRiskPremium\").as(\"riskPremium\"), col(\"FSScore\").as(\"score\")))\n",
    "    .withColumn(\"hf\", struct(col(\"HFRiskPremium\").as(\"riskPremium\"), col(\"HFScore\").as(\"score\")))\n",
    "    .withColumn(\"bd\", struct(col(\"BDRiskPremium\").as(\"riskPremium\"), col(\"BDScore\").as(\"score\")))\n",
    "    .withColumn(\"fn\", struct(col(\"FNRiskPremium\").as(\"riskPremium\"), col(\"FNScore\").as(\"score\")))\n",
    "    .withColumn(\"ns\", struct(col(\"NSRiskPremium\").as(\"riskPremium\"), col(\"NSScore\").as(\"score\")))\n",
    "    .withColumn(\"detail\", struct(col(\"fs\"), col(\"hf\"), col(\"bd\"), col(\"fn\"), col(\"ns\")))\n",
    "    .withColumn(\"rank\", struct(col(\"ReRank\"), col(\"kosdaq150\"), col(\"kospi200\"), col(\"krx100\"), col(\"krx300\")))\n",
    "    .select(\"corpCls\", \"stockCode\", \"updateDate\", \"bsnsYear\", \"quarter\", \"grade\", \"stockName\", \"riskPremium\", \"score\", \"detail\", \"rank\", \"loanAvailable\", \"basicReturn\", \"expectedProfit\", \"expectedRisk\", \"profitLoss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(\n",
    "    riskPremiumDf\n",
    "    .write.format(\"mongodb\")\n",
    "    .mode(\"append\")\n",
    "    .option(\"upsertDocument\", \"true\")\n",
    "    .option(\"idFieldList\", \"updateDate,stockCode\")\n",
    "    .option(\"spark.mongodb.read.connection.uri\", mongoUrl)\n",
    "    .option(\"spark.mongodb.write.connection.uri\", mongoUrl)\n",
    "    .option(\"database\", \"coreEngine\")\n",
    "    .option(\"collection\", \"RiskPremium\")\n",
    "    .save()\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
