{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T04:02:57.336051Z",
     "iopub.status.busy": "2024-03-15T04:02:57.335734Z",
     "iopub.status.idle": "2024-03-15T04:03:29.667860Z",
     "shell.execute_reply": "2024-03-15T04:03:29.667014Z",
     "shell.execute_reply.started": "2024-03-15T04:02:57.336025Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd6bbe6c3ba483b8643dc43293ed65d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n<tbody><tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>47</td><td>application_1707107681556_0048</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-15-66.ap-northeast-2.compute.internal:20888/proxy/application_1707107681556_0048/\" class=\"emr-proxy-link j-3F4C8SU6NE8DG application_1707107681556_0048\" emr-resource=\"j-3F4C8SU6NE8DG\n\" application-id=\"application_1707107681556_0048\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-1-206.ap-northeast-2.compute.internal:8042/node/containerlogs/container_1707107681556_0048_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.expressions._\n",
      "import scala.util.{Failure, Success, Try}\n",
      "import org.apache.spark.sql.types._\n",
      "import org.apache.spark.sql.SparkSession\n",
      "import com.mongodb.spark._\n",
      "import com.mongodb.spark.config._\n",
      "import org.bson._\n",
      "import java.time.LocalDateTime\n",
      "import java.time.LocalDate\n",
      "import java.time.format.DateTimeFormatter\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions._\n",
    "import scala.util.{Failure, Success, Try}\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import com.mongodb.spark._\n",
    "import com.mongodb.spark.config._\n",
    "import org.bson._\n",
    "\n",
    "import java.time.LocalDateTime\n",
    "import java.time.LocalDate\n",
    "import java.time.format.DateTimeFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark = (\n",
    "    SparkSession\n",
    "    .builder()\n",
    "    .appName(\"Frequency\")\n",
    "    .config(\"spark.cores.max\", 6)\n",
    "    .config(\"spark.executor.cores\", 6)\n",
    "    .config(\"spark.executor.memory\", \"36g\")\n",
    "    .getOrCreate()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateTimeFormatter = DateTimeFormatter.ofPattern(\"yyyyMMddHHmmss\")\n",
    "def appName: String = LocalDateTime.now().format(dateTimeFormatter)\n",
    "def previousDay(minusDays: Long) = {\n",
    "    LocalDateTime.now().minusDays(minusDays).format(dateTimeFormatter).substring(0, 8)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val mongoUrl = \"mongodb+srv://xxxxxxxxxxxxxxxxxxxxxx/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val matchPipe = String.format(\"{ $match: { updateDate: { $gte: '%s' } } }\", previousDay(120)) # %s는 정규식공백이 아니라, previousDay의 결과가 대체되는 것\n",
    "val df = (\n",
    "    spark.read.format(\"mongodb\")\n",
    "    .option(\"spark.mongodb.read.connection.uri\", mongoUrl)\n",
    "    .option(\"spark.mongodb.write.connection.uri\", mongoUrl)\n",
    "    .option(\"database\", \"coreEngine\")\n",
    "    .option(\"collection\", \"Price\")\n",
    "    .option(\"aggregation.pipeline\", matchPipe)\n",
    "    .load()\n",
    "    .drop(\"_id\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val nameFilter = udf((x: String) => {\n",
    "    val pattern = \"(금융|보험|기업인수목적|증권)\".r\n",
    "    pattern.findFirstIn(x) match {\n",
    "      case Some(s) => true\n",
    "      case None => false\n",
    "    }\n",
    "})\n",
    "\n",
    "\n",
    "val stockInfoDf = (\n",
    "    spark.read.format(\"mongodb\")\n",
    "    .option(\"spark.mongodb.read.connection.uri\", mongoUrl)\n",
    "    .option(\"spark.mongodb.write.connection.uri\", mongoUrl)\n",
    "    .option(\"database\", \"coreEngine\")\n",
    "    .option(\"collection\", \"StockInformation\")\n",
    "    .load()\n",
    "    .drop(\"_id\")\n",
    "    .where(col(\"stockType\") === \"보통주\")\n",
    "    .where(col(\"stockClassify\") !== \"부동산투자회사\")\n",
    "    .where(col(\"stockClassify\") !== \"주식예탁증권\")\n",
    "    .withColumn(\"regex\", nameFilter(col(\"stockFullName\"))) \n",
    "    .where(col(\"regex\") === false)\n",
    "    .select(\"stockCode\", \"stockType\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val mergeDf = df.join(stockInfoDf, Seq(\"stockCode\"), \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Event = (1번 상황 | 2번 상황) 이면 1\n",
    "Price(t)- price(t-1) <= price(t-1) * (-0.1)  즉, 직전 거래일 대비 주가가 10%이상 하락한 경우, \n",
    "직전 거래일과의 차이가 5일 연속으로 음수인 경우, 즉, 5거래일 동안 연속 하락한 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val window = Window.partitionBy(col(\"stockCode\")).orderBy(col(\"updateDate\"))\n",
    "// val rankWindow = Window.partitionBy(col(\"stockCode\")).orderBy(col(\"updateDate\").desc)\n",
    "val hf = (\n",
    "    mergeDf\n",
    "    .where(col(\"stockType\").isNotNull)\n",
    "    .where(col(\"closingPrice\").isNotNull)\n",
    "    .withColumn(\"market\", when(col(\"classify\") === \"KOSPI\", 1).otherwise(0))\n",
    "    .select(\"stockCode\", \"stockFullName\", \"market\", \"closingPrice\", \"prepare\", \"openPrice\", \"highPrice\", \"lowPrice\", \"tradingVolume\", \"transactionAmount\", \"updateDate\")\n",
    "    .withColumn(\"closingPricePrev\", lag(col(\"closingPrice\"), 1).over(window))\n",
    "    .withColumn(\"closingPriceNext\", lag(col(\"closingPrice\"), -1).over(window))\n",
    "    .withColumn(\"closingPriceDiff\", col(\"closingPrice\") - col(\"closingPricePrev\"))\n",
    "    .withColumn(\"closingPriceReturn\", log(col(\"closingPrice\") / col(\"closingPricePrev\")))\n",
    "    .withColumn(\"closingPriceEvt\", when(col(\"closingPriceDiff\") <= (col(\"closingPricePrev\") * -0.1), 1).otherwise(0))\n",
    "    .withColumn(\"closingPrice5Min\", min(col(\"closingPriceDiff\")).over(window.rowsBetween(-4, 0)))\n",
    "    .withColumn(\"closingPrice5Max\", max(col(\"closingPriceDiff\")).over(window.rowsBetween(-4, 0)))\n",
    "    .withColumn(\"closingPrice5\", when((col(\"closingPrice5Max\").cast(\"long\") < 0) && (col(\"closingPrice5Min\").cast(LongType) < 0), 1).otherwise(0))\n",
    "    .withColumn(\"tradingVolumeLog\", log(col(\"tradingVolume\")))\n",
    "    .withColumn(\"closingPriceReturn5days\", mean(col(\"closingPriceReturn\")).over(window.rowsBetween(-4, 0)))\n",
    "    .withColumn(\"closingPriceReturn22days\", mean(col(\"closingPriceReturn\")).over(window.rowsBetween(-21, 0)))\n",
    "    .withColumn(\"tradingVolumeVolatility5days\", (variance(col(\"tradingVolume\")).over(window.rowsBetween(-4, 0))))\n",
    "    .withColumn(\"tradingVolumeVolatility22days\", (variance(col(\"tradingVolume\")).over(window.rowsBetween(-21, 0))))\n",
    "    .withColumn(\"tradingVolumeVolatility5daysLog\", log(col(\"tradingVolume\")))\n",
    "    .withColumn(\"tradingVolumeVolatility22daysLog\", log(col(\"tradingVolume\")))\n",
    "    .withColumn(\"event\", when((col(\"closingPriceEvt\") === 1) || (col(\"closingPrice5\") === 1), 1).otherwise(0))\n",
    "    .select(\"stockCode\", \"stockFullName\", \"market\", \"updateDate\", \"closingPrice\", \"tradingVolume\", \"transactionAmount\", \"closingPricePrev\", \"closingPriceNext\", \"closingPriceDiff\", \"closingPriceReturn\", \"closingPrice5Min\", \"closingPrice5Max\", \"closingPrice5\", \"tradingVolumeLog\", \"closingPriceReturn5days\", \"closingPriceReturn22days\", \"tradingVolumeVolatility5days\", \"tradingVolumeVolatility22days\", \"tradingVolumeVolatility5daysLog\", \"tradingVolumeVolatility22daysLog\", \"event\")\n",
    "//     .withColumn(\"rank\", row_number().over(rankWindow))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(\n",
    "    hf\n",
    "    .where(col(\"updateDate\") > \"20230630\")\n",
    "    .na.fill(0)\n",
    "    .write.format(\"mongodb\")\n",
    "    .mode(\"append\")\n",
    "    .option(\"upsertDocument\", \"true\")\n",
    "    .option(\"idFieldList\", \"updateDate,stockCode\")\n",
    "    .option(\"spark.mongodb.read.connection.uri\", mongoUrl)\n",
    "    .option(\"spark.mongodb.write.connection.uri\", mongoUrl)\n",
    "    .option(\"database\", \"coreEngine\")\n",
    "    .option(\"collection\", \"PriceFeatures\")\n",
    "    .save()\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
