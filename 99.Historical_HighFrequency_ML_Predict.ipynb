{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"scipy==1.1.0\")\n",
    "sc.install_pypi_package(\"Cython==0.29.33\")\n",
    "sc.install_pypi_package(\"scikit-learn==0.24.2\")\n",
    "sc.install_pypi_package(\"pandas==0.23.2\")\n",
    "sc.install_pypi_package(\"s3fs==0.4.2\")\n",
    "sc.install_pypi_package(\"boto3==1.26.59\")\n",
    "sc.install_pypi_package(\"joblib==1.2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "import pickle\n",
    "import tempfile\n",
    "import joblib\n",
    "import io\n",
    "\n",
    "MongoUrl = \"mongodb+srv://xxxxxxxxxxxxxxxxxxxxxx/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Frequency\") \\\n",
    "    .config(\"spark.cores.max\", 6) \\\n",
    "    .config(\"spark.executor.cores\", 6) \\\n",
    "    .config(\"spark.executor.memory\", \"36g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"mongodb\") \\\n",
    "    .option(\"spark.mongodb.read.connection.uri\", MongoUrl) \\\n",
    "    .option(\"spark.mongodb.write.connection.uri\", MongoUrl) \\\n",
    "    .option(\"database\", \"coreEngine\") \\\n",
    "    .option(\"collection\", \"PriceFeatures\").load() \\\n",
    "    .drop(\"_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=\"ap-northeast-2\",\n",
    "    aws_access_key_id=\"xxxxxxxxxxxxxxxxxxxxxx\",\n",
    "    aws_secret_access_key=\"xxxxxxxxxxxxxxxxxxxxxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historicalDay = df.select(\"updateDate\").distinct().orderBy(\"updateDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historicalDayList = [i[0] for i in historicalDay.toPandas().values if i[0] > '20210101' and i[0] < '20221231' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.updateDate = hf.updateDate.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_files = s3.list_objects(Bucket=\"penta-engine\", Prefix=\"HighFrequencyModel/\")\n",
    "model_files = [i['Key'] for i in model_files['Contents']][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historicalDayList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for i in historicalDayList:\n",
    "    target_date = int(i)\n",
    "    model_file = s3.get_object(Bucket=\"penta-engine\", Key=f\"HighFrequencyModel/{target_date}.pkl\")\n",
    "    bytes_stream = io.BytesIO(model_file['Body'].read())\n",
    "    model = joblib.load(bytes_stream)\n",
    "\n",
    "    mask_test = hf.updateDate == target_date\n",
    "    df_test = hf.loc[mask_test,:].set_index([\"stockCode\", \"stockFullName\", \"updateDate\"])\n",
    "\n",
    "    df_test_set = df_test.drop(labels=\"event\", axis=1)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(df_test_set)\n",
    "    df_test_scaled = scaler.transform(df_test_set)\n",
    "    predict = model.predict_proba(df_test_scaled)\n",
    "\n",
    "    predict_df = pd.DataFrame(predict)\n",
    "    predict_df.index = df_test_set.index\n",
    "    predict_df['warningSignal'] = model.predict(df_test_set)\n",
    "\n",
    "\n",
    "    pdf = predict_df.reset_index()\n",
    "    pdf[\"stockCode\"] = pdf[\"stockCode\"].apply(lambda x: str(x).zfill(6))\n",
    "    pdf[\"updateDate\"] = pdf[\"updateDate\"].apply(str)\n",
    "    pdf.rename(columns={1: \"riskPremium\"}, inplace=True)\n",
    "    pdf.drop(0, axis=1, inplace=True)\n",
    "\n",
    "    dfs.append(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.concat(dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sdf \\\n",
    "    .write.format(\"mongodb\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"upsertDocument\", \"true\") \\\n",
    "    .option(\"idFieldList\", \"updateDate,stockCode\") \\\n",
    "    .option(\"spark.mongodb.read.connection.uri\", MongoUrl) \\\n",
    "    .option(\"spark.mongodb.write.connection.uri\", MongoUrl) \\\n",
    "    .option(\"database\", \"coreEngine\") \\\n",
    "    .option(\"collection\", \"PriceRiskPremium\") \\\n",
    "    .save()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
