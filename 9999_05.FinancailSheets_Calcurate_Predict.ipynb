{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"pybind11==2.10.3\")\n",
    "sc.install_pypi_package(\"numpy==1.19.0\")\n",
    "sc.install_pypi_package(\"Pillow==8.2\")\n",
    "sc.install_pypi_package(\"Cython==0.29.33\")\n",
    "sc.install_pypi_package(\"scipy==1.2.0\")\n",
    "sc.install_pypi_package(\"pythran==0.12.1\")\n",
    "sc.install_pypi_package(\"pandas==1.0.0\")\n",
    "sc.install_pypi_package(\"matplotlib==3.3.0\")\n",
    "sc.install_pypi_package(\"lifelines==0.27.4\")\n",
    "sc.install_pypi_package(\"s3fs==0.4.2\")\n",
    "sc.install_pypi_package(\"boto3==1.26.59\")\n",
    "sc.install_pypi_package(\"joblib==1.2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "from lifelines import CoxPHFitter\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "import pickle\n",
    "import tempfile\n",
    "import joblib\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "MongoUrl = \"mongodb+srv://xxxxxxxxxxxxxxxxxxxxxx/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"FinancialSheets_ML_Training\") \\\n",
    "    .config(\"spark.cores.max\", 6) \\\n",
    "    .config(\"spark.executor.cores\", 6) \\\n",
    "    .config(\"spark.executor.memory\", \"36g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reportRiskPremiumDf = spark.read.format(\"mongodb\") \\\n",
    "    .option(\"spark.mongodb.read.connection.uri\", MongoUrl) \\\n",
    "    .option(\"spark.mongodb.write.connection.uri\", MongoUrl) \\\n",
    "    .option(\"database\", \"coreEngine\") \\\n",
    "    .option(\"collection\", \"ReportRiskPremium\") \\\n",
    "    .load() \\\n",
    "    .drop(\"_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "priceDf = spark.read.format(\"mongodb\") \\\n",
    "    .option(\"spark.mongodb.read.connection.uri\", MongoUrl) \\\n",
    "    .option(\"spark.mongodb.write.connection.uri\", MongoUrl) \\\n",
    "    .option(\"database\", \"coreEngine\") \\\n",
    "    .option(\"collection\", \"Price\") \\\n",
    "    .load() \\\n",
    "    .drop(\"_id\") \\\n",
    "    .select(\"stockCode\", \"updateDate\", \"closingPrice\") \\\n",
    "    .withColumn('rolling', F.lag(F.col('closingPrice'), offset=90).over(Window.partitionBy('stockCode').orderBy(['updateDate']))) \\\n",
    "    .withColumn('closingPriceRolling', F.col('rolling') / F.col('closingPrice')) \\\n",
    "    .withColumn('r_s', F.when(F.col('closingPriceRolling') >= 1, 1 - 1).otherwise(F.col('closingPriceRolling') - 1)) \\\n",
    "    .withColumn('r', F.lit(0.07)) \\\n",
    "    .withColumn('recoveryFN1', F.lit(0.5)) \\\n",
    "    .withColumn('recoveryFN2', F.lit(0.1)) \\\n",
    "    .withColumn('priceEvent', F.when(F.col('r_s') < -0.2, 1).otherwise(0)) \\\n",
    "    .withColumn('priceEvent', F.sum(F.col('priceEvent')).over(Window.partitionBy('stockCode').orderBy('updateDate').rowsBetween(-90, 0))) \\\n",
    "    .withColumn('priceEvent', F.when(F.col('priceEvent') > 0, 1).otherwise(0)) \\\n",
    "    .drop('closingPrice', 'closingPriceRolling', 'rolling') \\\n",
    "    .where(F.col(\"r_s\").isNotNull())\n",
    "    \n",
    "    \n",
    "# matchPipe = \"{ $match: { updateDate: { $gte: '20200101' } } }\"\n",
    "#     .option(\"aggregation.pipeline\", matchPipe) \\\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priceDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reportRiskPremiumDfPriceDf = reportRiskPremiumDf \\\n",
    "    .join(priceDf, on=[\"stockCode\", \"updateDate\"], how=\"left\") \\\n",
    "    .withColumn('TF1', F.sum(F.col('event')).over(Window.partitionBy('stockCode').orderBy(['updateDate'])) / F.count(F.col('event')).over(Window.partitionBy('stockCode').orderBy(['updateDate']))) \\\n",
    "    .withColumn('TF2', F.sum(F.col('priceEvent')).over(Window.partitionBy('stockCode').orderBy(['updateDate'])) / F.count(F.col('priceEvent')).over(Window.partitionBy('stockCode').orderBy(['updateDate']))) \\\n",
    "    .withColumn('FSPctRank', F.percent_rank().over(Window.partitionBy(\"bsnsYear\", \"quarter\").orderBy(\"riskPremium\"))) \\\n",
    "    .withColumn(\"expectedProfit\", F.lit(100000000 * 90/365) * F.col(\"r\")) \\\n",
    "    .withColumn(\"expectedLossFN1\", F.col(\"expectedProfit\") * F.col(\"recoveryFN1\") * F.col(\"TF1\")) \\\n",
    "    .withColumn(\"expectedLossFN2\", F.col(\"expectedProfit\") * F.col(\"recoveryFN2\") * F.col(\"TF2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = reportRiskPremiumDfPriceDf.toPandas()\n",
    "periodDf = pdf[['updateDate']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold(prob, df):\n",
    "    df = df.fillna(0)    \n",
    "    tn = (df['event'] == False).values * (prob == False).values # 현실 양성 | 예측 양성 \n",
    "    fn1 = (df['event'] == True).values * (prob == False).values # 현실 악성 | 예측 양성 \n",
    "    fn2 = (df['priceEvent'] == True).values * (prob == False).values # 수정주가 20 현실 악성 | 예측 양성\n",
    "    tp = (df['event'] == True).values * (prob == True).values # 현실 악성 | 예측 악성\n",
    "    fp = (df['event'] == False).values * (prob == True).values # 현실 양성 | 예측 악성\n",
    "\n",
    "    x1 = df['expectedProfit'] @ tn\n",
    "    x2 = df['expectedLossFN1'] @ fn1\n",
    "    x3 = df['expectedLossFN2'] @ fn2\n",
    "    x4 = df['expectedLossFN1'] @ tp\n",
    "    x5 = df['expectedProfit'] @ fp\n",
    "    \n",
    "    exret = (x1 - x2 - x4 + x5) / sum(df['expectedProfit'])\n",
    "\n",
    "    return exret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([p[0] for p in list(periodDf.values) if p[0] > '20210101'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_dfs = []\n",
    "\n",
    "for i in sorted([p[0] for p in list(periodDf.values) if p[0] > '20210101']):\n",
    "    tmp_df = pdf[(pdf['updateDate'] == i)].reset_index()\n",
    "    metric = lambda x: get_threshold(prob=(tmp_df['FSPctRank'] > x), df=tmp_df)\n",
    "    res = minimize(lambda x: -metric(x[0]), 0.3, method='nelder-mead', options={'disp': True})\n",
    "    threshold = res.x[0]\n",
    "    tmp_df['threshold'] = threshold\n",
    "    tmp_df['predict'] = (tmp_df['FSPctRank'] >= tmp_df['threshold']).astype(int)\n",
    "    threshold_dfs.append(tmp_df)\n",
    "\n",
    "threshold_df = pd.concat(threshold_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tdf in threshold_dfs:\n",
    "    sdf = spark.createDataFrame(tdf) \\\n",
    "        .withColumn(\"grade\", F.lit(\"D\")) \\\n",
    "        .withColumn(\"grade\", F.when(F.col(\"predict\") >= 1, F.lit(\"F\")).otherwise(F.col(\"grade\"))) \\\n",
    "        .withColumn(\"grade\", F.when(F.col(\"FSPctRank\") <= 0.40, F.lit(\"C\")).otherwise(F.col(\"grade\"))) \\\n",
    "        .withColumn(\"grade\", F.when(F.col(\"FSPctRank\") <= 0.20, F.lit(\"B\")).otherwise(F.col(\"grade\"))) \\\n",
    "        .withColumn(\"grade\", F.when(F.col(\"FSPctRank\") <= 0.10, F.lit(\"A\")).otherwise(F.col(\"grade\"))) \\\n",
    "        .withColumn(\"TT\", F.lit(0.07)) \\\n",
    "        .withColumn(\"TT\", F.when(F.col(\"grade\") == \"B\", 0.075).otherwise(F.col(\"TT\"))) \\\n",
    "        .withColumn(\"TT\", F.when(F.col(\"grade\") == \"C\", 0.085).otherwise(F.col(\"TT\"))) \\\n",
    "        .withColumn(\"TT\", F.when(F.col(\"grade\") == \"D\", 0.095).otherwise(F.col(\"TT\"))) \\\n",
    "        .withColumn(\"TT\", F.when(F.col(\"grade\") == \"F\", 0.1).otherwise(F.col(\"TT\"))) \\\n",
    "        .withColumn(\"basicReturn\", F.col(\"TT\") * F.lit((90.0 / 365.0) * 100000000.0)) \\\n",
    "        .withColumn(\"basicReturn\", F.when(F.col(\"grade\") == \"A\", F.col(\"basicReturn\").cast('Integer') * (F.lit(0.9) - F.col(\"FSPctRank\"))).otherwise(F.col(\"basicReturn\"))) \\\n",
    "        .withColumn(\"basicReturn\", F.when(F.col(\"grade\") == \"B\", F.col(\"basicReturn\").cast('Integer') * (F.lit(0.8) - F.col(\"FSPctRank\"))).otherwise(F.col(\"basicReturn\"))) \\\n",
    "        .withColumn(\"basicReturn\", F.when(F.col(\"grade\") == \"C\", F.col(\"basicReturn\").cast('Integer') * (F.lit(0.7) - F.col(\"FSPctRank\"))).otherwise(F.col(\"basicReturn\"))) \\\n",
    "        .withColumn(\"basicReturn\", F.when(F.col(\"grade\") == \"D\", F.col(\"basicReturn\").cast('Integer') * (F.lit(0.5) - F.col(\"FSPctRank\"))).otherwise(F.col(\"basicReturn\"))) \\\n",
    "        .withColumn(\"basicReturn\", F.when(F.col(\"grade\") == \"F\", F.col(\"basicReturn\").cast('Integer') * (F.lit(0.0) - F.col(\"FSPctRank\"))).otherwise(F.col(\"basicReturn\"))) \\\n",
    "        .withColumn(\"VaRTF1\", F.col(\"TF1\") * F.lit((90.0 / 365.0) * (100000000.0 * 0.5 * 0.08))) \\\n",
    "        .withColumn(\"VaRTF2\", F.col(\"TF2\") * F.lit((90.0 / 365.0) * (100000000.0 * 0.1 * 0.08))) \\\n",
    "        .withColumn(\"expectedRisk\", F.col(\"VaRTF1\") + F.col(\"VaRTF1\")) \\\n",
    "        .withColumn(\"profitLoss\", F.col(\"basicReturn\") - F.col(\"expectedRisk\")) \\\n",
    "        .withColumn(\"loanAvailable\", F.when(F.col(\"FSPctRank\") < F.col(\"threshold\"), 1).otherwise(0)) \\\n",
    "        .drop(\"index\", \"TT\", \"TF1\", \"TF2\", \"r_s\", \"r\", \"priceEvent\", \"recoveryFN1\", \"recoveryFN2\", \"expectedLossFN1\", \"expectedLossFN2\")\n",
    "\n",
    "    sdf \\\n",
    "        .write.format(\"mongodb\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"upsertDocument\", \"true\") \\\n",
    "        .option(\"idFieldList\", \"updateDate,stockCode\") \\\n",
    "        .option(\"spark.mongodb.read.connection.uri\", MongoUrl) \\\n",
    "        .option(\"spark.mongodb.write.connection.uri\", MongoUrl) \\\n",
    "        .option(\"database\", \"coreEngine\") \\\n",
    "        .option(\"collection\", \"ReportRiskPremiumCalcurate\") \\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_dfs = []\n",
    "\n",
    "for i in sorted([p[0] for p in list(periodDf.values) if p[0] < '20210101']):\n",
    "    tmp_df = pdf[(pdf['updateDate'] == i)].reset_index()\n",
    "    metric = lambda x: get_threshold(prob=(tmp_df['FSPctRank'] > x), df=tmp_df)\n",
    "    res = minimize(lambda x: -metric(x[0]), 0.3, method='nelder-mead', options={'disp': True})\n",
    "    threshold = res.x[0]\n",
    "    tmp_df['threshold'] = threshold\n",
    "    tmp_df['predict'] = (tmp_df['FSPctRank'] >= tmp_df['threshold']).astype(int)\n",
    "    threshold_dfs.append(tmp_df)\n",
    "\n",
    "threshold_df = pd.concat(threshold_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tdf in threshold_dfs:\n",
    "    sdf = spark.createDataFrame(tdf) \\\n",
    "        .withColumn(\"grade\", F.lit(\"D\")) \\\n",
    "        .withColumn(\"grade\", F.when(F.col(\"predict\") >= 1, F.lit(\"F\")).otherwise(F.col(\"grade\"))) \\\n",
    "        .withColumn(\"grade\", F.when(F.col(\"FSPctRank\") <= 0.40, F.lit(\"C\")).otherwise(F.col(\"grade\"))) \\\n",
    "        .withColumn(\"grade\", F.when(F.col(\"FSPctRank\") <= 0.20, F.lit(\"B\")).otherwise(F.col(\"grade\"))) \\\n",
    "        .withColumn(\"grade\", F.when(F.col(\"FSPctRank\") <= 0.10, F.lit(\"A\")).otherwise(F.col(\"grade\"))) \\\n",
    "        .withColumn(\"TT\", F.lit(0.07)) \\\n",
    "        .withColumn(\"TT\", F.when(F.col(\"grade\") == \"B\", 0.075).otherwise(F.col(\"TT\"))) \\\n",
    "        .withColumn(\"TT\", F.when(F.col(\"grade\") == \"C\", 0.085).otherwise(F.col(\"TT\"))) \\\n",
    "        .withColumn(\"TT\", F.when(F.col(\"grade\") == \"D\", 0.095).otherwise(F.col(\"TT\"))) \\\n",
    "        .withColumn(\"TT\", F.when(F.col(\"grade\") == \"F\", 0.1).otherwise(F.col(\"TT\"))) \\\n",
    "        .withColumn(\"basicReturn\", F.col(\"TT\") * F.lit((90.0 / 365.0) * 100000000.0)) \\\n",
    "        .withColumn(\"basicReturn\", F.when(F.col(\"grade\") == \"A\", F.col(\"basicReturn\").cast('Integer') * (F.lit(0.9) - F.col(\"FSPctRank\"))).otherwise(F.col(\"basicReturn\"))) \\\n",
    "        .withColumn(\"basicReturn\", F.when(F.col(\"grade\") == \"B\", F.col(\"basicReturn\").cast('Integer') * (F.lit(0.8) - F.col(\"FSPctRank\"))).otherwise(F.col(\"basicReturn\"))) \\\n",
    "        .withColumn(\"basicReturn\", F.when(F.col(\"grade\") == \"C\", F.col(\"basicReturn\").cast('Integer') * (F.lit(0.7) - F.col(\"FSPctRank\"))).otherwise(F.col(\"basicReturn\"))) \\\n",
    "        .withColumn(\"basicReturn\", F.when(F.col(\"grade\") == \"D\", F.col(\"basicReturn\").cast('Integer') * (F.lit(0.5) - F.col(\"FSPctRank\"))).otherwise(F.col(\"basicReturn\"))) \\\n",
    "        .withColumn(\"basicReturn\", F.when(F.col(\"grade\") == \"F\", F.col(\"basicReturn\").cast('Integer') * (F.lit(0.0) - F.col(\"FSPctRank\"))).otherwise(F.col(\"basicReturn\"))) \\\n",
    "        .withColumn(\"VaRTF1\", F.col(\"TF1\") * F.lit((90.0 / 365.0) * (100000000.0 * 0.5 * 0.08))) \\\n",
    "        .withColumn(\"VaRTF2\", F.col(\"TF2\") * F.lit((90.0 / 365.0) * (100000000.0 * 0.1 * 0.08))) \\\n",
    "        .withColumn(\"expectedRisk\", F.col(\"VaRTF1\") + F.col(\"VaRTF1\")) \\\n",
    "        .withColumn(\"profitLoss\", F.col(\"basicReturn\") - F.col(\"expectedRisk\")) \\\n",
    "        .withColumn(\"loanAvailable\", F.when(F.col(\"FSPctRank\") < F.col(\"threshold\"), 1).otherwise(0)) \\\n",
    "        .drop(\"index\", \"TT\", \"TF1\", \"TF2\", \"r_s\", \"r\", \"priceEvent\", \"recoveryFN1\", \"recoveryFN2\", \"expectedLossFN1\", \"expectedLossFN2\")\n",
    "\"\"\"\n",
    "    sdf \\\n",
    "        .write.format(\"mongodb\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"upsertDocument\", \"true\") \\\n",
    "        .option(\"idFieldList\", \"updateDate,stockCode\") \\\n",
    "        .option(\"spark.mongodb.read.connection.uri\", MongoUrl) \\\n",
    "        .option(\"spark.mongodb.write.connection.uri\", MongoUrl) \\\n",
    "        .option(\"database\", \"coreEngine\") \\\n",
    "        .option(\"collection\", \"ReportRiskPremiumCalcurate\") \\\n",
    "        .save()\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
