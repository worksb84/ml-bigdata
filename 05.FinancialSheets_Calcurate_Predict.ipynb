{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"pybind11==2.10.3\")\n",
    "sc.install_pypi_package(\"numpy==1.19.0\")\n",
    "sc.install_pypi_package(\"Pillow==8.2\")\n",
    "sc.install_pypi_package(\"Cython==0.29.33\")\n",
    "sc.install_pypi_package(\"scipy==1.2.0\")\n",
    "sc.install_pypi_package(\"pythran==0.12.1\")\n",
    "sc.install_pypi_package(\"pandas==1.0.0\")\n",
    "sc.install_pypi_package(\"matplotlib==3.3.0\")\n",
    "sc.install_pypi_package(\"lifelines==0.27.4\")\n",
    "sc.install_pypi_package(\"s3fs==0.4.2\")\n",
    "sc.install_pypi_package(\"boto3==1.26.59\")\n",
    "sc.install_pypi_package(\"joblib==1.2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "from lifelines import CoxPHFitter\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "import pickle\n",
    "import tempfile\n",
    "import joblib\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MongoUrl = \"mongodb+srv://xxxxxxxxxxxxxxxxxxxxxx/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"FinancialSheets_ML_Training\") \\\n",
    "    .config(\"spark.cores.max\", 6) \\\n",
    "    .config(\"spark.executor.cores\", 6) \\\n",
    "    .config(\"spark.executor.memory\", \"36g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reportRiskPremiumDf = spark.read.format(\"mongodb\") \\\n",
    "    .option(\"spark.mongodb.read.connection.uri\", MongoUrl) \\\n",
    "    .option(\"spark.mongodb.write.connection.uri\", MongoUrl) \\\n",
    "    .option(\"database\", \"coreEngine\") \\\n",
    "    .option(\"collection\", \"ReportRiskPremium_New\") \\\n",
    "    .load() \\\n",
    "    .drop(\"_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchPipe = \"{ $match: { updateDate: { $gte: '20200101' } } }\"\n",
    "    \n",
    "priceDf = spark.read.format(\"mongodb\") \\\n",
    "    .option(\"spark.mongodb.read.connection.uri\", MongoUrl) \\\n",
    "    .option(\"spark.mongodb.write.connection.uri\", MongoUrl) \\\n",
    "    .option(\"aggregation.pipeline\", matchPipe) \\\n",
    "    .option(\"database\", \"coreEngine\") \\\n",
    "    .option(\"collection\", \"Price\") \\\n",
    "    .load() \\\n",
    "    .drop(\"_id\") \\\n",
    "    .select(\"stockCode\", \"updateDate\", \"closingPrice\",\"marketCap\") \\\n",
    "    .withColumn('rolling', F.lag(F.col('closingPrice'), offset=90).over(Window.partitionBy('stockCode').orderBy(['updateDate']))) \\\n",
    "    .withColumn('closingPriceRolling', F.col('rolling') / F.col('closingPrice')) \\\n",
    "    .withColumn('r_s', F.when(F.col('closingPriceRolling') >= 1, 1 - 1).otherwise(F.col('closingPriceRolling') - 1)) \\\n",
    "    .withColumn('r', F.lit(0.07)) \\\n",
    "    .withColumn('recoveryFN1', F.lit(0.5)) \\\n",
    "    .withColumn('recoveryFN2', F.lit(0.1)) \\\n",
    "    .withColumn('recoveryFN3', F.lit(0.1)) \\\n",
    "    .withColumn('priceEvent', F.when(F.col('r_s') < -0.2, 1).otherwise(0)) \\\n",
    "    .withColumn('priceEvent', F.sum(F.col('priceEvent')).over(Window.partitionBy('stockCode').orderBy('updateDate').rowsBetween(-90, 0))) \\\n",
    "    .withColumn('priceEvent', F.when(F.col('priceEvent') > 0, 1).otherwise(0)) \\\n",
    "    .drop('closingPrice', 'closingPriceRolling', 'rolling') \\\n",
    "    .where(F.col(\"r_s\").isNotNull())\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reportRiskPremiumDfPriceDf = reportRiskPremiumDf \\\n",
    "    .join(priceDf, on=[\"stockCode\", \"updateDate\"], how=\"left\") \\\n",
    "    .withColumn('TF1', F.sum(F.col('event')).over(Window.partitionBy('stockCode').orderBy(['updateDate'])) / F.count(F.col('event')).over(Window.partitionBy('stockCode').orderBy(['updateDate']))) \\\n",
    "    .withColumn('TF2', F.sum(F.col('priceEvent')).over(Window.partitionBy('stockCode').orderBy(['updateDate'])) / F.count(F.col('priceEvent')).over(Window.partitionBy('stockCode').orderBy(['updateDate']))) \\\n",
    "    .withColumn('TF3', F.sum(F.col('plbtEvent')).over(Window.partitionBy('stockCode').orderBy(['updateDate'])) / F.count(F.col('plbtEvent')).over(Window.partitionBy('stockCode').orderBy(['updateDate']))) \\\n",
    "    .withColumn(\"expectedProfit\", F.lit(100000000 * 90/365) * F.col(\"r\")) \\\n",
    "    .withColumn(\"expectedLossFN1\", F.col(\"expectedProfit\") * F.col(\"recoveryFN1\") * F.col(\"TF1\")) \\\n",
    "    .withColumn(\"expectedLossFN2\", F.col(\"expectedProfit\") * F.col(\"recoveryFN2\") * F.col(\"TF2\"))\\\n",
    "    .withColumn(\"expectedLossFN3\", F.col(\"expectedProfit\") * F.col(\"recoveryFN3\") * F.col(\"TF3\"))\\\n",
    "\n",
    "#     .withColumn('FSPctRank', F.percent_rank().over(Window.partitionBy(\"bsnsYear\", \"quarter\").orderBy(\"riskPremium\"))) \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = reportRiskPremiumDfPriceDf.toPandas()\n",
    "periodDf = pdf[['updateDate']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list=sorted([p[0] for p in list(periodDf.values) if p[0] > '20210101'])\n",
    "\n",
    "pdf_for_group = [] # 인스턴스가 누적으로 쌓이는 df의 리스트\n",
    "\n",
    "'''\n",
    "1. date_list = ['20220414', '20220427', '20220428', '20220502', ...] \n",
    "    =>  i보다 작은 것중 가장 큰 것을 가져옴으로 4월 14일보다 작은 값 중에서 가장 큰 값은 4월 11일임. (만약 4월 11일이 있다면)\n",
    "\n",
    "\n",
    "<for문 과정>\n",
    "    1번째\n",
    "    i = 20220414\n",
    "    updateDate <= 20220414. 애들 중에서 stockcode로 그룹화하여 각 기업별로 가장 최신의 것 하나.\n",
    "\n",
    "\n",
    "    2번째 \n",
    "    i = 20220427\n",
    "    updateDate <= 20220427. 애들 중에서 stockcode로 그룹화하여 각 기업별로 가장 최신의 것 하나.\n",
    "        =>따라서 1번째도 포함하여, 누적으로 df에 인스턴스가 쌓임\n",
    "\n",
    "        \n",
    "    마지막\n",
    "    i == 20230814\n",
    "    기업별 가장 최신의 값들만 있는 df가 됨.\n",
    "\n",
    "'''\n",
    "\n",
    "for i in date_list:\n",
    "    temp_df = pdf[pdf['updateDate'] <= i].sort_values(by=\"updateDate\", ascending=False).groupby(\"stockCode\").head(1)\n",
    "    temp_df[\"tempUpdateDate\"]=i\n",
    "    pdf_for_group.append(temp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict  </br>\n",
    "<pre>\n",
    "    1. 과정을 통해 날짜별로 가장 최신의 df를 만듦.</br>\n",
    "        - pdf_for_group 기준일에서 회사별 최신 정보로 업데이트된 df를 리스트로 묶음. 날짜별로 df가 리스트로 저장된 것.</br>\n",
    "        - 각 df에서 riskPremium의 rank를 백분율로 표시 => FSPctRank 칼럼</br>\n",
    "    2. get_threshold 함수</br>\n",
    "        - 혼동행렬 수치로 얼마나 우리가 정의한 악재를 잘 구분하는지 계산.</br>\n",
    "        - 최적화 통해 기준값 x를 산출 (minimize 함수 사용) => 긍정, 부정을 가장 잘 구분해내는 기준값 x\n",
    "    3. predict 칼럼 생성\n",
    "        - FSPctRank 가 x보다크면 1, 아니면 0 \n",
    "        - 0이면 우량기업 일 확률 높음.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in pdf_for_group:\n",
    "    df['FSPctRank'] = df['riskPremium'].transform('rank', pct=True) #순위가 아니라백분율로 표시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적의 threshold x 찾기\n",
    "\n",
    "* x보다 크면 => true => 1 > 악재 \n",
    "* x보다 작으면 > Faslse > 0>호재\n",
    "\n",
    " 최적화 함수 minize 함수를 metric이라는 변수에 저장. <br>\n",
    " 그 결과값을 res에 저장하여 theshold를 찾는다.\n",
    "\n",
    "## predict\n",
    "> FSPctRank가 threshold보다 크면 True > ***predict = 1*** > 악재 <br>\n",
    "> 작으면 False >  ***predict = 0***  > 호재\n",
    "\n",
    "\n",
    "## 등급 기준\n",
    "<pre>\n",
    "    predict가 0인 기업들은 대출 가능 기업이고\n",
    "        grade : A~D \n",
    "    predict가 1인 기업들은 악재발생한 기업이므로\n",
    "        grade : F\n",
    "    \n",
    "    </pre>\n",
    "## reGrade process\n",
    "![a1](images/pngreGrade_rePredict.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_dfs = []\n",
    "\n",
    "for i in pdf_for_group:\n",
    "    \n",
    "    def get_threshold(prob, df):\n",
    "        df = df.fillna(0)    \n",
    "        tn = (df['event'] == False).values * (prob == False).values # 현실 양성 | 예측 양성 \n",
    "        fn1 = (df['event'] == True).values * (prob == False).values # 현실 악성 | 예측 양성 \n",
    "        fn2 = (df['priceEvent'] == True).values * (prob == False).values # 수정주가 20 현실 악성 | 예측 양성   => 90일이 아니고?\n",
    "        fn3 = (df['plbtEvent'] == True).values * (prob == False).values # 당기순이익 2분기 연속 악성 | 예측 양성\n",
    "        tp = (df['event'] == True).values * (prob == True).values # 현실 악성 | 예측 악성\n",
    "        fp = (df['event'] == False).values * (prob == True).values # 현실 양성 | 예측 악성\n",
    "\n",
    "        x1 = df['expectedProfit'] @ tn\n",
    "        x2 = df['expectedLossFN1'] @ fn1\n",
    "        x3 = df['expectedLossFN2'] @ fn2\n",
    "        x4 = df['expectedLossFN1'] @ tp\n",
    "        x5 = df['expectedProfit'] @ fp\n",
    "\n",
    "#         x6 = df['expectedLossFN3'] @ fn3\n",
    "#         exret = (x1 - x2 - x4 + x5) / sum(df['expectedProfit'])\n",
    "#         exret = (x1 - x2 - x3 - x4 - x6 + x5) / sum(df['expectedProfit'])\n",
    "        exret = (x1 - x2 - x4 + x5) / sum(df['expectedProfit'])\n",
    "        return exret\n",
    "    \n",
    "\n",
    "    tmp_df = i.reset_index(drop=True)\n",
    "    median = np.median(tmp_df['FSPctRank'].values)\n",
    "    print(median)\n",
    "    metric = lambda x: get_threshold(prob=(tmp_df['FSPctRank'] > x), df=tmp_df)  #기준 x를 최적화함\n",
    "    res = minimize(lambda x: -metric(x[0]), median, method='nelder-mead', options={'disp': False}) \n",
    "    threshold = res.x[0]\n",
    "    tmp_df['threshold'] = threshold\n",
    "    tmp_df['predict'] = (tmp_df['FSPctRank'] >= tmp_df['threshold']).astype(int)\n",
    "    threshold_dfs.append(tmp_df)\n",
    "\n",
    "threshold_dfs\n",
    "\n",
    "'''\n",
    "1. metric = lambda x: get_threshold(prob=(tmp_df['FSPctRank'] > x), df=tmp_df)\n",
    "    metric이라는 것은 lambda로 정의된 함수임. 그래서 출력하면 lambda function으로 메모리에 저장되어 있음\n",
    "\n",
    "\n",
    "2. minimize는 최적화 함수. \n",
    "    minimize(함수, 초기 추정값, method=  , options = )\n",
    "    반환되는 값은 최적화된 변수를 반환. 변수가 여러개일 수 있으므로 리스트 형태로 반환됨=> x[0]인 이유 // 우리는 변수가 하나이므로 [x]의 형태임. 리스트 벗기려고 인덱스 사용하는 것\n",
    "\n",
    "    <method 종류>\n",
    "        'Nelder-Mead': Simplex algorithm (downhill simplex algorithm). 적은 차원의 문제에 유용합니다. 계산시간이 적거나 소규모인 문제, 노이즈가 있는 경우, 제한된 메모리 등에서 유용\n",
    "        'Powell': Powell’s method. Conjugate direction method w0ith successive quadratic programming. 대칭 행렬을 다루는 데 효과적입니다.\n",
    "        'CG': Conjugate Gradient algorithm. 특히 대칭, 양의 정부호 행렬에 대한 최적화에 효과적입니다.\n",
    "        'BFGS': Broyden-Fletcher-Goldfarb-Shanno algorithm. 큰 규모의 최적화 문제에서 효과적입니다.\n",
    "        'Newton-CG': Newton-Conjugate Gradient algorithm. Hessian matrix를 이용하여 적은 메모리를 사용하면서도 빠르게 최적화할 수 있습니다.\n",
    "        'L-BFGS-B': Limited-memory Broyden-Fletcher-Goldfarb-Shanno with bounds. BFGS와 유사하나, 제한 조건이 있는 경우에 사용됩니다.\n",
    "        'TNC': Truncated Newton-Conjugate Gradient algorithm. 제한된 최적화에 사용되며, 제한 조건이 있는 경우 효과적입니다.\n",
    "        'COBYLA': Constrained Optimization BY Linear Approximations algorithm. 제한된 최적화에 사용되며, 제한 조건이 선형일 때 효과적입니다.\n",
    "        'SLSQP': Sequential Least SQuares Programming. 비선형 최소화와 제한 조건이 있는 최적화에 사용됩니다.\n",
    "        'trust-constr': Trust-region constrained algorithm. 제한된 최적화 문제를 다루며, 선형 또는 비선형 제한 조건을 포함할 수 있습니다.\n",
    "        'trust-exact': Trust-region exact algorithm. 정확한 Hessian matrix를 이용하여 최적화합니다.\n",
    "        'trust-krylov': Trust-region Krylov algorithm. 큰 규모의 비선형 제한 조건을 다룹니다.\n",
    "\n",
    "    <option 종류>\n",
    "        maxiter: 최대 반복 횟수를 지정합니다. 최적화 알고리즘이 이 횟수만큼 반복한 후 종료됩니다.\n",
    "        disp: 최적화 알고리즘의 진행 상황을 표시할지 여부를 지정합니다. True로 설정하면 출력이 표시되고, False로 설정하면 출력이 표시되지 않습니다.\n",
    "        ftol: 함수 값이 충분히 수렴되면 종료하는데 사용되는 상대적인 허용 오차입니다.\n",
    "        gtol: 그래디언트의 크기가 충분히 작아지면 종료하는데 사용되는 상대적인 허용 오차입니다.\n",
    "        xtol: 변수의 변화량이 충분히 작아지면 종료하는데 사용되는 상대적인 허용 오차입니다.\n",
    "        maxfun: 최대 함수 호출 횟수를 지정합니다. 목적 함수의 평가가 이 횟수를 초과하면 최적화 알고리즘이 종료됩니다.\n",
    "        maxfev: 최대 그래디언트 평가 횟수를 지정합니다.\n",
    "        eps: 수치 그래디언트의 수치적 안정성을 제어하는 작은 양수입니다.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_dfs = []\n",
    "import copy\n",
    "for i in threshold_dfs:  \n",
    "    tdf = i[~i.stockCode.isin(['012700','023460','334890','293940','396690','377190','365550','369370','400760','404990','064820','088260','350520','348950','395400','204210','145270'])]\n",
    "#  제외 기업들은 리츠사임\n",
    "# ~ : not 연산자\n",
    "    loanAvailableDf = tdf[tdf['predict'] == 0]\n",
    "    loanAvailableDf['loanFSPctRank'] = loanAvailableDf['riskPremium'].transform('rank', pct=True)\n",
    "    loanAvailableDf['grade'] = 'D'\n",
    "    loanAvailableDf.loc[(loanAvailableDf['loanFSPctRank'] <= 0.8, 'grade')] = 'C'\n",
    "    loanAvailableDf.loc[(loanAvailableDf['loanFSPctRank'] <= 0.5, 'grade')] = 'B'\n",
    "    loanAvailableDf.loc[(loanAvailableDf['loanFSPctRank'] <= 0.2, 'grade')] = 'A'\n",
    "    loanAvailableDf.drop(\"loanFSPctRank\", axis=1, inplace=True)\n",
    "    loanNotAvailableDf = tdf[tdf['predict'] == 1]\n",
    "    loanNotAvailableDf['grade'] = 'F'\n",
    "    temp_df = pd.concat([loanAvailableDf, loanNotAvailableDf]).sort_values(by=\"FSPctRank\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "    temp_grade = copy.deepcopy(temp_df['grade'].values)\n",
    "    \n",
    "    temp_df['rank'] = temp_df['FSPctRank'].rank(method='first')\n",
    "    temp_df.loc[(temp_df['plbtEvent'] == 1), 'tmpRank'] = temp_df['rank'] + 50.5  # 최근 2분기 합계 영업이익이 <0 이하인 경우 가중치 \n",
    "\n",
    "    temp_df.loc[(temp_df['plbtEvent'] == 0), 'tmpRank'] = temp_df['rank']\n",
    "    temp_df['reRank'] = temp_df['tmpRank'].rank(method='first')\n",
    "    temp_df = temp_df.sort_values(by=\"reRank\", ascending=True)\n",
    "    temp_df.drop(\"tmpRank\", axis=1, inplace=True)\n",
    "    temp_df['reGrade'] = temp_grade\n",
    "    temp_df.loc[(temp_df['grade'] == 'F'), 'reGrade'] = 'F'\n",
    "    \n",
    "    temp_df.loc[(temp_df['reGrade'] == 'F'), 'rePredict'] = 1\n",
    "    temp_df.loc[(temp_df['reGrade'] != 'F'), 'rePredict'] = 0\n",
    "    \n",
    "    rank_dfs.append(temp_df)\n",
    "\n",
    "\n",
    "    '''\n",
    "    rank(method = )  디폴트는 average\n",
    "        ranking을 어떻게 나타내줄지를 결정하는 옵션입니다.\n",
    "        min -> 동일한 rank일 경우 숫자가 더 작은(= 1위에 가까운) rank를 return\n",
    "        max -> 동일한 rank일 경우 숫자가 더 큰(= 1위로부터 먼) rank를 return\n",
    "        dense -> 동일한 rank 뒤에 나오는 rank를 바로 +1을 더한 rank로 표시함. (동일한 rank 다음 나오는 rank를 건너뛰고 표시하지 않음.)\n",
    "        first -> 동일한 rank를 같은 순위로 표시하지 않고, 원본 data에서 더 위쪽에 있는 행에 숫자가 더 작은(= 1위에 가까운) rank를 부여함. 따라서 이 옵션은 rank에 따른 row number를 매길 수 있음.\n",
    "        average -> first 옵션과 동일하지만 동일한 수치를 가져서 동일 rank인 행들에 대해 rank의 평균값을 부여함.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.concat(rank_dfs).reset_index(drop=True)\n",
    "sdf2 = spark.createDataFrame(dfs) \\\n",
    "        .withColumn(\"loanAvailable\", F.col(\"predict\")) \\\n",
    "        .withColumn(\"reLoanAvailable\", F.col(\"rePredict\")) \\\n",
    "        .withColumn(\"TT\", F.lit(0.07)) \\\n",
    "        .withColumn(\"TT\", F.when(F.col(\"grade\") == \"B\", 0.075).otherwise(F.col(\"TT\"))) \\\n",
    "        .withColumn(\"TT\", F.when(F.col(\"grade\") == \"C\", 0.085).otherwise(F.col(\"TT\"))) \\\n",
    "        .withColumn(\"TT\", F.when(F.col(\"grade\") == \"D\", 0.095).otherwise(F.col(\"TT\"))) \\\n",
    "        .withColumn(\"TT\", F.when(F.col(\"grade\") == \"F\", 0.1).otherwise(F.col(\"TT\"))) \\\n",
    "        .withColumn(\"basicReturn\", F.col(\"TT\") * F.lit((90.0 / 365.0) * 100000000.0)) \\\n",
    "        .withColumn(\"basicReturn\", F.when(F.col(\"grade\") == \"A\", F.col(\"basicReturn\").cast('Integer') * (F.lit(0.9) - F.col(\"FSPctRank\"))).otherwise(F.col(\"basicReturn\"))) \\\n",
    "        .withColumn(\"basicReturn\", F.when(F.col(\"grade\") == \"B\", F.col(\"basicReturn\").cast('Integer') * (F.lit(0.8) - F.col(\"FSPctRank\"))).otherwise(F.col(\"basicReturn\"))) \\\n",
    "        .withColumn(\"basicReturn\", F.when(F.col(\"grade\") == \"C\", F.col(\"basicReturn\").cast('Integer') * (F.lit(0.7) - F.col(\"FSPctRank\"))).otherwise(F.col(\"basicReturn\"))) \\\n",
    "        .withColumn(\"basicReturn\", F.when(F.col(\"grade\") == \"D\", F.col(\"basicReturn\").cast('Integer') * (F.lit(0.5) - F.col(\"FSPctRank\"))).otherwise(F.col(\"basicReturn\"))) \\\n",
    "        .withColumn(\"basicReturn\", F.when(F.col(\"grade\") == \"F\", F.col(\"basicReturn\").cast('Integer') * (F.lit(0.0) - F.col(\"FSPctRank\"))).otherwise(F.col(\"basicReturn\"))) \\\n",
    "        .withColumn(\"VaRTF1\", F.col(\"TF1\") * F.lit((90.0 / 365.0) * (100000000.0 * 0.5 * 0.08))) \\\n",
    "        .withColumn(\"VaRTF2\", F.col(\"TF2\") * F.lit((90.0 / 365.0) * (100000000.0 * 0.1 * 0.08))) \\\n",
    "        .withColumn(\"VaRTF3\", F.col(\"TF3\") * F.lit((90.0 / 365.0) * (100000000.0 * 0.1 * 0.08))) \\\n",
    "        .withColumn(\"expectedRisk\", F.col(\"VaRTF1\") + F.col(\"VaRTF2\") + F.col(\"VaRTF3\")) \\\n",
    "        .withColumn(\"profitLoss\", F.col(\"basicReturn\") - F.col(\"expectedRisk\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sdf2 \\\n",
    "    .where(F.col(\"updateDate\") > \"20230530\") \\\n",
    "    .write.format(\"mongodb\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"upsertDocument\", \"true\") \\\n",
    "    .option(\"idFieldList\", \"updateDate,tempUpdateDate,stockCode\") \\\n",
    "    .option(\"spark.mongodb.read.connection.uri\", MongoUrl) \\\n",
    "    .option(\"spark.mongodb.write.connection.uri\", MongoUrl) \\\n",
    "    .option(\"database\", \"coreEngine\") \\\n",
    "    .option(\"collection\", \"ReportRiskPremiumCalcurate_New\") \\\n",
    "    .save()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
