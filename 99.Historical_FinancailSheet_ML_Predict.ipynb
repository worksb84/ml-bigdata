{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"pybind11==2.10.3\")\n",
    "sc.install_pypi_package(\"numpy==1.19.0\")\n",
    "sc.install_pypi_package(\"Pillow==8.2\")\n",
    "sc.install_pypi_package(\"Cython==0.29.33\")\n",
    "sc.install_pypi_package(\"scipy==1.2.0\")\n",
    "sc.install_pypi_package(\"pythran==0.12.1\")\n",
    "sc.install_pypi_package(\"pandas==1.0.0\")\n",
    "sc.install_pypi_package(\"matplotlib==3.3.0\")\n",
    "sc.install_pypi_package(\"lifelines==0.27.4\")\n",
    "sc.install_pypi_package(\"s3fs==0.4.2\")\n",
    "sc.install_pypi_package(\"boto3==1.26.59\")\n",
    "sc.install_pypi_package(\"joblib==1.2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "from lifelines import CoxPHFitter\n",
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "import pickle\n",
    "import tempfile\n",
    "import joblib\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MongoUrl = \"mongodb+srv://xxxxxxxxxxxxxxxxxxxxxx/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"FinancialSheets_ML_Training\") \\\n",
    "    .config(\"spark.cores.max\", 6) \\\n",
    "    .config(\"spark.executor.cores\", 6) \\\n",
    "    .config(\"spark.executor.memory\", \"36g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"mongodb\") \\\n",
    "    .option(\"spark.mongodb.read.connection.uri\", MongoUrl) \\\n",
    "    .option(\"spark.mongodb.write.connection.uri\", MongoUrl) \\\n",
    "    .option(\"database\", \"coreEngine\") \\\n",
    "    .option(\"collection\", \"ReportFeatures\") \\\n",
    "    .load() \\\n",
    "    .drop(\"_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historicalDay = df.select(\"updateDate\").distinct().orderBy(\"updateDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historicalDayList = [i[0] for i in historicalDay.toPandas().values if i[0] > '20210506'] #20180101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_features = features\n",
    "hf_features[\"event\"] = hf_features[\"event\"].fillna(0).astype(bool)\n",
    "hf_features = hf_features.sort_values([\"stockCode\", \"rceptNo\"])\n",
    "hf_features = hf_features[hf_features[\"period\"] >= 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"CACL\", \"CATA\", \"CLCA\", \"CLGR\", \"CLTL\", \"DSR01\", \"DSR02\", \"DSR03\", \n",
    "    \"DSR04\", \"DSR05\", \"DSR06\", \"DSR07\", \"EBTIN\", \"EQTA\", \"FAGR\", \n",
    "    \"FFOEQ\", \"FFOTL\", \"INSL\", \"INTL\", \"LNSL\", \"LNTA\", \"MB\", \"NEGBE\", \n",
    "    \"NIGR\", \"NISL\", \"RETA\", \"SLEQ\", \"SLFA\", \"TLEQ\", \"TLTA\"]\n",
    "\n",
    "for column in columns:\n",
    "    hf_features[column]=hf_features.groupby([\"bsnsYear\", \"reprtCode\"])[column].apply(lambda x:x.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=\"ap-northeast-2\",\n",
    "    aws_access_key_id=\"xxxxxxxxxxxxxxxxxxxxxx\",\n",
    "    aws_secret_access_key=\"xxxxxxxxxxxxxxxxxxxxxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_files = s3.list_objects(Bucket=\"penta-engine\", Prefix=\"FinancialSheetsModel/\")\n",
    "model_files = [i['Key'] for i in model_files['Contents']][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_features = hf_features.fillna(0).dropna()\n",
    "hf_features.updateDate = hf_features.updateDate.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for i in model_files:\n",
    "    print(i)\n",
    "    \n",
    "    i = \"FinancialSheetsModel/20230530.pkl\"\n",
    "    \n",
    "    target_date = int(i[-12:][:8])\n",
    "    df_test_set = hf_features[hf_features[\"updateDate\"] <= target_date].sort_values(by=\"updateDate\", ascending=False).drop_duplicates().groupby(\"stockCode\").head(1).sort_values(by=[\"stockCode\", \"updateDate\"], ascending=True).reset_index(drop=True)\n",
    "    df_test_set = df_test_set.set_index([\"stockCode\", \"corpCls\", \"corpCode\", \"reprtCode\", \"rceptNo\", \"stockName\", \"updateDate\", \"bsnsYear\", \"quarter\"])\n",
    "    \n",
    "    model_file = s3.get_object(Bucket=\"penta-engine\", Key=i)\n",
    "    bytes_stream = io.BytesIO(model_file['Body'].read())\n",
    "    model = joblib.load(bytes_stream)\n",
    "    hazard = model.predict_partial_hazard(df_test_set.drop('event', axis=1))\n",
    "    \n",
    "    df_test_set['riskPremium'] = hazard\n",
    "    result = df_test_set.reset_index()\n",
    "    result = result[['stockCode','corpCls','corpCode','reprtCode','rceptNo','stockName','updateDate','bsnsYear','quarter','riskPremium','event']]\n",
    "    result['event'] = result['event'].astype('int')\n",
    "    result['updateDate'] = str(target_date)\n",
    "    dfs.append(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = \"FinancialSheetsModel/20230504.pkl\"\n",
    "\n",
    "target_date = 20210506\n",
    "df_test_set = hf_features[hf_features[\"updateDate\"] <= target_date].sort_values(by=\"updateDate\", ascending=False).drop_duplicates().groupby(\"stockCode\").head(1).sort_values(by=[\"stockCode\", \"updateDate\"], ascending=True).reset_index(drop=True)\n",
    "df_test_set = df_test_set.set_index([\"stockCode\", \"corpCls\", \"corpCode\", \"reprtCode\", \"rceptNo\", \"stockName\", \"updateDate\", \"bsnsYear\", \"quarter\"])\n",
    "\n",
    "model_file = s3.get_object(Bucket=\"penta-engine\", Key=i)\n",
    "bytes_stream = io.BytesIO(model_file['Body'].read())\n",
    "model = joblib.load(bytes_stream)\n",
    "hazard = model.predict_partial_hazard(df_test_set.drop('event', axis=1))\n",
    "\n",
    "df_test_set['riskPremium'] = hazard\n",
    "result = df_test_set.reset_index()\n",
    "result = result[['stockCode','corpCls','corpCode','reprtCode','rceptNo','stockName','updateDate','bsnsYear','quarter','riskPremium','event']]\n",
    "result['event'] = result['event'].astype('int')\n",
    "result['updateDate'] = str(target_date)\n",
    "# dfs.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.sort_values(\"riskPremium\").head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.concat(dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf['corpCls'] = pdf['corpCls'].apply(lambda x:  'K' if x == 0 else 'Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.where(F.col(\"stockCode\") == \"005930\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(\n",
    "    sdf\n",
    "    .write.format(\"mongodb\")\n",
    "    .mode(\"append\")\n",
    "    .option(\"upsertDocument\", \"true\")\n",
    "    .option(\"idFieldList\", \"updateDate,stockCode,rceptNo\")\n",
    "    .option(\"spark.mongodb.read.connection.uri\", MongoUrl)\n",
    "    .option(\"spark.mongodb.write.connection.uri\", MongoUrl)\n",
    "    .option(\"database\", \"coreEngine\")\n",
    "    .option(\"collection\", \"ReportRiskPremium\")\n",
    "    .save()\n",
    "    )\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.count() # 1498888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
