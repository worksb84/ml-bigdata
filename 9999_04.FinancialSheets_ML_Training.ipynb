{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"pybind11==2.10.3\")\n",
    "sc.install_pypi_package(\"numpy==1.19.0\")\n",
    "sc.install_pypi_package(\"Pillow==8.2\")\n",
    "sc.install_pypi_package(\"Cython==0.29.33\")\n",
    "sc.install_pypi_package(\"scipy==1.2.0\")\n",
    "sc.install_pypi_package(\"pythran==0.12.1\")\n",
    "sc.install_pypi_package(\"pandas==1.0.0\")\n",
    "sc.install_pypi_package(\"matplotlib==3.3.0\")\n",
    "sc.install_pypi_package(\"lifelines==0.27.4\")\n",
    "sc.install_pypi_package(\"s3fs==0.4.2\")\n",
    "sc.install_pypi_package(\"boto3==1.26.59\")\n",
    "sc.install_pypi_package(\"joblib==1.2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "from lifelines import CoxPHFitter\n",
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "import pickle\n",
    "import tempfile\n",
    "import joblib\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "MongoUrl = \"mongodb+srv://xxxxxxxxxxxxxxxxxxxxxx/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"FinancialSheets_ML_Training\") \\\n",
    "    .config(\"spark.cores.max\", 6) \\\n",
    "    .config(\"spark.executor.cores\", 6) \\\n",
    "    .config(\"spark.executor.memory\", \"36g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"mongodb\") \\\n",
    "    .option(\"spark.mongodb.read.connection.uri\", MongoUrl) \\\n",
    "    .option(\"spark.mongodb.write.connection.uri\", MongoUrl) \\\n",
    "    .option(\"database\", \"coreEngine\") \\\n",
    "    .option(\"collection\", \"ReportFeatures_temp\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_features = features.drop([\"_id\"], axis=1)\n",
    "hf_features[\"event\"] = hf_features[\"event\"].fillna(0).astype(bool)\n",
    "hf_features = hf_features.sort_values([\"stockCode\", \"rceptNo\"])\n",
    "hf_features = hf_features[hf_features[\"period\"] >= 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"CACL\", \"CATA\", \"CLCA\", \"CLGR\", \"CLTL\", \"DSR01\", \"DSR02\", \"DSR03\", \n",
    "    \"DSR04\", \"DSR05\", \"DSR06\", \"DSR07\", \"DSR08\", \"EBTIN\", \"EQTA\", \"FAGR\", \n",
    "    \"FFOEQ\", \"FFOTL\", \"INSL\", \"INTL\", \"LNSL\", \"LNTA\", \"MB\", \"NEGBE\", \n",
    "    \"NIGR\", \"NISL\", \"RETA\", \"SLEQ\", \"SLFA\", \"TLEQ\", \"TLTA\"]\n",
    "\n",
    "for column in columns:\n",
    "    hf_features[column]=hf_features.groupby([\"bsnsYear\", \"reprtCode\"])[column].apply(lambda x:x.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_features = hf_features.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundry = \"2020\"\n",
    "\n",
    "df_train_set = hf_features[hf_features[\"bsnsYear\"] <= boundry].drop_duplicates()\n",
    "df_train_set = df_train_set.set_index([\"stockCode\", \"corpCls\", \"corpCode\", \"reprtCode\", \"rceptNo\", \"stockName\", \"updateDate\", \"bsnsYear\", \"quarter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CoxPHFitter(penalizer=0.0001, l1_ratio=1)\n",
    "model.fit(df_train_set, duration_col=\"period\", event_col=\"event\", fit_options=dict(step_size=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=\"ap-northeast-2\",\n",
    "    aws_access_key_id=\"xxxxxxxxxxxxxxxxxxxxxx\",\n",
    "    aws_secret_access_key=\"xxxxxxxxxxxxxxxxxxxxxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "f = io.BytesIO()\n",
    "joblib.dump(model, f)\n",
    "f.seek(0)\n",
    "s3.put_object(Bucket=\"penta-engine\", Key=\"FinancialSheetsModel.pkl\", Body=f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
