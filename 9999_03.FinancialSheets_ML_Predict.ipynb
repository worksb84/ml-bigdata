{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"pybind11==2.10.3\")\n",
    "sc.install_pypi_package(\"numpy==1.19.0\")\n",
    "sc.install_pypi_package(\"Pillow==8.2\")\n",
    "sc.install_pypi_package(\"Cython==0.29.33\")\n",
    "sc.install_pypi_package(\"scipy==1.2.0\")\n",
    "sc.install_pypi_package(\"pythran==0.12.1\")\n",
    "sc.install_pypi_package(\"pandas==1.0.0\")\n",
    "sc.install_pypi_package(\"matplotlib==3.3.0\")\n",
    "sc.install_pypi_package(\"lifelines==0.27.4\")\n",
    "sc.install_pypi_package(\"s3fs==0.4.2\")\n",
    "sc.install_pypi_package(\"boto3==1.26.59\")\n",
    "sc.install_pypi_package(\"joblib==1.2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "from lifelines import CoxPHFitter\n",
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "import pickle\n",
    "import tempfile\n",
    "import joblib\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MongoUrl = \"mongodb+srv://xxxxxxxxxxxxxxxxxxxxxx/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"FinancialSheets_ML_Training\") \\\n",
    "    .config(\"spark.cores.max\", 6) \\\n",
    "    .config(\"spark.executor.cores\", 6) \\\n",
    "    .config(\"spark.executor.memory\", \"36g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"mongodb\") \\\n",
    "    .option(\"spark.mongodb.read.connection.uri\", MongoUrl) \\\n",
    "    .option(\"spark.mongodb.write.connection.uri\", MongoUrl) \\\n",
    "    .option(\"database\", \"coreEngine\") \\\n",
    "    .option(\"collection\", \"ReportFeatures_temp\") \\\n",
    "    .load() \\\n",
    "    .drop(\"_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_features = features\n",
    "hf_features[\"event\"] = hf_features[\"event\"].fillna(0).astype(bool)\n",
    "hf_features = hf_features.sort_values([\"stockCode\", \"rceptNo\"])\n",
    "hf_features = hf_features[hf_features[\"period\"] >= 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"CACL\", \"CATA\", \"CLCA\", \"CLGR\", \"CLTL\", \"DSR01\", \"DSR02\", \"DSR03\", \n",
    "    \"DSR04\", \"DSR05\", \"DSR06\", \"DSR07\", \"DSR08\", \"EBTIN\", \"EQTA\", \"FAGR\", \n",
    "    \"FFOEQ\", \"FFOTL\", \"INSL\", \"INTL\", \"LNSL\", \"LNTA\", \"MB\", \"NEGBE\", \n",
    "    \"NIGR\", \"NISL\", \"RETA\", \"SLEQ\", \"SLFA\", \"TLEQ\", \"TLTA\"]\n",
    "\n",
    "for column in columns:\n",
    "    hf_features[column]=hf_features.groupby([\"bsnsYear\", \"reprtCode\"])[column].apply(lambda x:x.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_features = hf_features.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundry = \"2023\"\n",
    "\n",
    "df_test_set = hf_features[hf_features[\"bsnsYear\"] > boundry].drop_duplicates()\n",
    "df_test_set = df_test_set.set_index([\"stockCode\", \"corpCls\", \"corpCode\", \"reprtCode\", \"rceptNo\", \"stockName\", \"updateDate\", \"bsnsYear\", \"quarter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=\"ap-northeast-2\",\n",
    "    aws_access_key_id=\"xxxxxxxxxxxxxxxxxxxxxx\",\n",
    "    aws_secret_access_key=\"xxxxxxxxxxxxxxxxxxxxxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = s3.get_object(Bucket=\"penta-engine\", Key=\"FinancialSheetsModelTemp.pkl\")\n",
    "bytes_stream = io.BytesIO(model_file['Body'].read())\n",
    "model = joblib.load(bytes_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard = model.predict_partial_hazard(df_test_set.drop('event', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_set['riskPremium'] = hazard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df_test_set.reset_index()\n",
    "result = result[['stockCode','corpCls','corpCode','reprtCode','rceptNo','stockName','updateDate','bsnsYear','quarter','riskPremium','event']]\n",
    "result['event'] = result['event'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.createDataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(\n",
    "    sdf\n",
    "    .write.format(\"mongodb\")\n",
    "    .mode(\"append\")\n",
    "    .option(\"upsertDocument\", \"true\")\n",
    "    .option(\"idFieldList\", \"updateDate,stockCode,rceptNo\")\n",
    "    .option(\"spark.mongodb.read.connection.uri\", MongoUrl)\n",
    "    .option(\"spark.mongodb.write.connection.uri\", MongoUrl)\n",
    "    .option(\"database\", \"coreEngine\")\n",
    "    .option(\"collection\", \"ReportRiskPremium\")\n",
    "    .save()\n",
    "    )\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
