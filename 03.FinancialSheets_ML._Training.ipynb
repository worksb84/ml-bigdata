{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"pybind11==2.10.3\")\n",
    "sc.install_pypi_package(\"numpy==1.19.0\")\n",
    "sc.install_pypi_package(\"Pillow==8.2\")\n",
    "sc.install_pypi_package(\"Cython==0.29.33\")\n",
    "sc.install_pypi_package(\"scipy==1.2.0\")\n",
    "sc.install_pypi_package(\"pythran==0.12.1\")\n",
    "sc.install_pypi_package(\"pandas==1.0.0\")\n",
    "sc.install_pypi_package(\"matplotlib==3.3.0\")\n",
    "sc.install_pypi_package(\"lifelines==0.27.4\")\n",
    "sc.install_pypi_package(\"s3fs==0.4.2\")\n",
    "sc.install_pypi_package(\"boto3==1.26.59\")\n",
    "sc.install_pypi_package(\"joblib==1.2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "from lifelines import CoxPHFitter\n",
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "import pickle\n",
    "import tempfile\n",
    "import joblib\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "MongoUrl = \"mongodb+srv://xxxxxxxxxxxxxxxxxxxxxx/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"FinancialSheets_ML_Training\") \\\n",
    "    .config(\"spark.cores.max\", 6) \\\n",
    "    .config(\"spark.executor.cores\", 6) \\\n",
    "    .config(\"spark.executor.memory\", \"36g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"mongodb\") \\\n",
    "    .option(\"spark.mongodb.read.connection.uri\", MongoUrl) \\\n",
    "    .option(\"spark.mongodb.write.connection.uri\", MongoUrl) \\\n",
    "    .option(\"database\", \"coreEngine\") \\\n",
    "    .option(\"collection\", \"ReportFeaturesTest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_features = features.drop([\"_id\"], axis=1)\n",
    "hf_features[\"event\"] = hf_features[\"event\"].fillna(0).astype(bool)\n",
    "hf_features = hf_features.sort_values([\"stockCode\", \"rceptNo\"])\n",
    "hf_features = hf_features[hf_features[\"period\"] >= 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "            \"DSR01\", \"DSR02\", \"DSR03\", \"DSR04\", \"DSR05\",\n",
    "            \"CLTL\", \"EBTIN2\", \"INTL2\", \"LNSL\", \"LNTA\", \"MB2\", \"NIGR2\", \"NISL\", \"RETA\", \n",
    "            \"SLEQ\", \"SLFA\", \"TLEQ\"]\n",
    "#             \"FFOEQ\", \"FFOTL\", \"CACL\", \"EQTA\", \"INSL\", \"CATA\", \"TLTA\", \"INTL\", \n",
    "#             \"MB\", \"NIGR\", \"FAGR\", \"EBTIN\", \"CLCA\", \"NEGBE\", \"CLGR\", \n",
    "         \n",
    "\n",
    "    \n",
    "for column in columns:\n",
    "    hf_features[column]=hf_features.groupby([\"bsnsYear\", \"reprtCode\"])[column].apply(lambda x:x.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_features = hf_features.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundry = \"2018\"\n",
    "boudnry2 = \"2024\"\n",
    "indexColumn = [\"stockCode\", \"corpCls\", \"corpCode\", \"reprtCode\", \"rceptNo\", \"stockName\", \"updateDate\", \"bsnsYear\", \"quarter\"]\n",
    "\n",
    "df_train_set = hf_features[(hf_features[\"bsnsYear\"] >= boundry) & (hf_features[\"bsnsYear\"] < boudnry2)].drop_duplicates()\n",
    "df_train_set = df_train_set.set_index(indexColumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CoxPHFitter(penalizer=0.0001, l1_ratio=1)  #pernalizer가 0인 것은 정규화 없는 모델, 0보다 클수록 정규화 효과 강화. 모델을 더 일반화 시킨다는 것.\n",
    "model.fit(df_train_set, duration_col=\"period\", event_col=\"event\", fit_options=dict(step_size=0.2))\n",
    "\n",
    "# period 는 estDt부터 재무제표 등록 일자까지의 기간\n",
    "\n",
    "'''\n",
    "** event 변수는 '법인세차감전이익'(t) + '법인세차감전이익'(t-1) < 0 을 떄 1,\n",
    "\n",
    "val disclosure  #몽고db에서 불러와서 disclosure라는 데이터프레임 생성.\n",
    "    spark.read.format(\"mongodb\")\n",
    "    .withColumn(\"event\", lit(1)) #event 컬럼 생성하고 1로 초기화\n",
    "\n",
    "val fs_features    #fs_preprocess 데이터프레임으로 새로운 데이터프레임을 만드는 것.\n",
    "    fs_preprocess   \n",
    "    .withColumn(\"event\", when(col(\"ProfitLossBeforeTax\") + lag(col(\"ProfitLossBeforeTax\"), offset=1).over(partition)<0, 1).otherwise(col(\"event\")))\n",
    "\n",
    "---------------------------------------\n",
    "cox 비례 위험 모형\n",
    "event : 생존/사망 또는 잔류/사퇴와 같은 관심 경험 // 우리는 최근 2분기의 영업이익의 합계가 <0 이거나 불성실공시인 경우.\n",
    "생존시간 : event가 발생할때까지의 기간 // 우리는 설립일부터 updateDate까지의 기간\n",
    "\n",
    "hazard : t-1에 살았던 사람이 t시점에 죽을 확률\n",
    "H(t) = hazard 누적  => 일정하지 않고. 시간에 따라 변함\n",
    "따라서 \n",
    "1. log rank test (x가 하나일 때 주로 사용)\n",
    "    HR(t) = H(위험인자 노출군)/H(비노출군)  => 평균은 HRMC라고 하고 이게 1이면, 시험군과 대조근의 차이가 없는 것.  \n",
    "\n",
    "2. cox's propotional hazard medel\n",
    "    HR(t)는 시간에 대해 일정하다고 가정.\n",
    "    이 가정에 맞지 않는 데이터라면 결과 왜곡 가능성 높음.\n",
    "\n",
    "    ##정확한 이해를 위해 통계학 수업 또는 유튜브 설명 듣기!!\n",
    "    \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=\"ap-northeast-2\",\n",
    "    aws_access_key_id=\"xxxxxxxxxxxxxxxxxxxxxx\",\n",
    "    aws_secret_access_key=\"xxxxxxxxxxxxxxxxxxxxxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "f = io.BytesIO()\n",
    "joblib.dump(model, f)\n",
    "f.seek(0)\n",
    "s3.put_object(Bucket=\"penta-engine\", Key=\"FinancialSheetsModelNew.pkl\", Body=f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
